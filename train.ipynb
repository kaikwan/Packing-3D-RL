{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import datetime\n",
    "\n",
    "from QLearning.agent import QLearning\n",
    "from common.plot import plot_rewards\n",
    "from common.utils import save_results,make_dir\n",
    "curr_time = datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\") # obtain current time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QlearningConfig:\n",
    "    '''训练相关参数'''\n",
    "    def __init__(self):\n",
    "        self.algo = 'Qlearning'\n",
    "        # self.env = 'CliffWalking-v0' # 0 up, 1 right, 2 down, 3 left\n",
    "        # path to save results\n",
    "        self.result_path = \"/outputs/\" + curr_time + '/results/'\n",
    "        # path to save models\n",
    "        self.model_path = \"/outputs/\" + curr_time + '/models/'\n",
    "        \n",
    "        self.train_eps = 200        # 训练的episode数目\n",
    "        self.eval_eps = 30          # 评估的episode数目\n",
    "        self.gamma = 0.9            # reward的衰减率\n",
    "        self.epsilon_start = 0.95   # e-greedy策略中初始epsilon\n",
    "        self.epsilon_end = 0.01     # e-greedy策略中的终止epsilon\n",
    "        self.epsilon_decay = 200    # e-greedy策略中epsilon的衰减率\n",
    "        self.lr = 0.1               # learning rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# seed 表示环境的随机性\n",
    "def env_agent_config(cfg, seed=1):\n",
    "    # env = CliffWalkingWapper(env)\n",
    "    env = \n",
    "    # Q Table 的行数\n",
    "    state_dim = env.observation_space.n\n",
    "    # Q Table 的列数\n",
    "    action_dim = env.action_space.n\n",
    "    agent = QLearning(state_dim, action_dim, cfg)\n",
    "    return env,agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(cfg, env, agent):\n",
    "    rewards = []  \n",
    "    ma_rewards = [] # moving average reward\n",
    "\n",
    "    for i_ep in range(cfg.train_eps):\n",
    "        \n",
    "        # 记录每个episode的reward\n",
    "        ep_reward = 0  \n",
    "        # 重置环境, 重新开一局（即开始新的一个episode）\n",
    "        state = env.reset()  \n",
    "\n",
    "        while True:\n",
    "            # 根据算法选择一个动作\n",
    "            action = agent.choose_action(state)  \n",
    "            # 与环境进行一次动作交互，获得该动作导致的下一个动作和回报\n",
    "            next_state, reward, done, _ = env.step(action)\n",
    "            # Q-learning 算法更新 agent\n",
    "            agent.update(state, action, reward, next_state, done) \n",
    "\n",
    "            state = next_state  # 存储上一个观察值\n",
    "            ep_reward += reward\n",
    "            if done:\n",
    "                break\n",
    "\n",
    "        rewards.append(ep_reward)\n",
    "        # 平滑回报曲线\n",
    "        if ma_rewards:\n",
    "            ma_rewards.append(ma_rewards[-1] * 0.9 + ep_reward * 0.1)\n",
    "        else:\n",
    "            ma_rewards.append(ep_reward)\n",
    "        if (i_ep + 1) % 10 == 0:\n",
    "            print(\"Episode:{}/{}: reward:{:.1f}\".format(i_ep + 1, cfg.train_eps,ep_reward))\n",
    "\n",
    "    return rewards, ma_rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg = QlearningConfig()\n",
    "env,agent = env_agent_config(cfg, seed=1)\n",
    "rewards,ma_rewards = train(cfg, env, agent)\n",
    "make_dir(cfg.result_path,cfg.model_path)\n",
    "agent.save(path=cfg.model_path)\n",
    "save_results(rewards,ma_rewards,tag='train',path=cfg.result_path)\n",
    "plot_rewards(rewards,ma_rewards,tag=\"train\",env=cfg.env,algo = cfg.algo,path=cfg.result_path)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
